using StatsFuns
using DistributionsAD
using KernelFunctions
using Random: AbstractRNG, GLOBAL_RNG


struct SteinDistribution{T,M<:AbstractMatrix{T}} <: Distributions.ContinuousMultivariateDistribution
    dim::Int
    n_particles::Int
    x::M
end

length(d::SteinDistribution) = d.dim
eltype(::SteinDistribution{T}) where {T} = T
mean(d::SteinDistribution) = mean(d.x, dims=2)
cov(d::SteinDistribution) = cov(d.x, dims=2)

"""
    SteinVI(n_particles = 100, max_iters = 1000)

Stein Variational Inference (SteinVI) for a given model.
"""
struct SteinVI{AD} <: VariationalInference{AD}
    max_iters::Int        # maximum number of gradient steps used in optimization
    kernel::Kernel
end

# params(alg::SteinVI) = nothing;#params(alg.kernel)

SteinVI(args...) = SteinVI{ADBackend()}(args...)
SteinVI() = SteinVI(100, SqExponentialKernel())

alg_str(::SteinVI) = "SteinVI"

function vi(model, alg::SteinVI, q::SteinDistribution; optimizer = TruncatedADAGrad())
    DEBUG && @debug "Optimizing SteinVI..."
    # Initial parameters for mean-field approx
    Î¸ = [q.x]#params(alg)

    # Optimize
    optimize!(elbo, alg, q, model, Î¸; optimizer = optimizer)

    # Return updated `Distribution`
    return q
end

function optimize!(
    elbo::ELBO,
    alg::SteinVI,
    q::SteinDistribution,
    model,
    Î¸;
    optimizer = TruncatedADAGrad(),
)
    alg_name = alg_str(alg)
    max_iters = alg.max_iters

end
function vi(model, alg::SteinVI, nParticles, kernel; optimizer = TruncatedADAGrad())
    q = SteinDistribution(nParticles, kernel)
    DEBUG && @debug "Optimizing SteinVI..."
    Î¸ = copy(Î¸_init)
    optimize!(elbo, alg, q, model, Î¸; optimizer = optimizer)

    # If `q` is a mean-field approx we use the specialized `update` function
    if q isa Distribution
        return update(q, Î¸)
    else
        # Otherwise we assume it's a mapping Î¸ â†’ q
        return q(Î¸)
    end
end


function optimize(elbo::ELBO, alg::SteinVI, q, model, Î¸_init; optimizer = TruncatedADAGrad())
    Î¸ = copy(Î¸_init)

    # `model` assumed to be callable z â†¦ p(x, z)
    optimize!(elbo, alg, q, model, Î¸; optimizer = optimizer)

    return Î¸
end

function (elbo::ELBO)(
    rng::AbstractRNG,
    alg::SteinVI,
    q::VariationalPosterior,
    logÏ€,
    num_samples
)
    #   ð”¼_q(z)[log p(xáµ¢, z)]
    # = âˆ« log p(xáµ¢, z) q(z) dz
    # = âˆ« log p(xáµ¢, f(Ï•)) q(f(Ï•)) |det J_f(Ï•)| dÏ•   (since change of variables)
    # = âˆ« log p(xáµ¢, f(Ï•)) qÌƒ(Ï•) dÏ•                   (since q(f(Ï•)) |det J_f(Ï•)| = qÌƒ(Ï•))
    # = ð”¼_qÌƒ(Ï•)[log p(xáµ¢, z)]

    #   ð”¼_q(z)[log q(z)]
    # = âˆ« q(f(Ï•)) log (q(f(Ï•))) |det J_f(Ï•)| dÏ•     (since q(f(Ï•)) |det J_f(Ï•)| = qÌƒ(Ï•))
    # = ð”¼_qÌƒ(Ï•) [log q(f(Ï•))]
    # = ð”¼_qÌƒ(Ï•) [log qÌƒ(Ï•) - log |det J_f(Ï•)|]
    # = ð”¼_qÌƒ(Ï•) [log qÌƒ(Ï•)] - ð”¼_qÌƒ(Ï•) [log |det J_f(Ï•)|]
    # = - â„(qÌƒ(Ï•)) - ð”¼_qÌƒ(Ï•) [log |det J_f(Ï•)|]

    # Finally, the ELBO is given by
    # ELBO = ð”¼_q(z)[log p(xáµ¢, z)] - ð”¼_q(z)[log q(z)]
    #      = ð”¼_qÌƒ(Ï•)[log p(xáµ¢, z)] + ð”¼_qÌƒ(Ï•) [log |det J_f(Ï•)|] + â„(qÌƒ(Ï•))

    # If f: supp(p(z | x)) â†’ â„ then
    # ELBO = ð”¼[log p(x, z) - log q(z)]
    #      = ð”¼[log p(x, fâ»Â¹(zÌƒ)) + logabsdet(J(fâ»Â¹(zÌƒ)))] + â„(qÌƒ(zÌƒ))
    #      = ð”¼[log p(x, z) - logabsdetjac(J(f(z)))] + â„(qÌƒ(zÌƒ))

    # But our `forward(q)` is using fâ»Â¹: â„ â†’ supp(p(z | x)) going forward â†’ `+ logjac`
    _, z, logjac, _ = forward(rng, q)
    res = (logÏ€(z) + logjac) / num_samples

    res += (q isa TransformedDistribution) ? entropy(q.dist) : entropy(q)

    for i = 2:num_samples
        _, z, logjac, _ = forward(rng, q)
        res += (logÏ€(z) + logjac) / num_samples
    end

    return res
end
